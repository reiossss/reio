# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

"""
yoloæ£€æµ‹æ¡†/åˆ†å‰²æ¡†æ•°æ®å¢å¼ºè„šæœ¬ï¼Œå®ç°æ—‹è½¬ã€å¹³ç§»ã€ç¼©æ”¾ã€å·¦å³ç¿»è½¬ã€äº®åº¦ã€é¥±å’Œåº¦ç­‰ä¸€ç³»åˆ—éšæœºæ··åˆå¢å¼ºï¼Œé»˜è®¤ä½¿æ•°æ®é›†æ‰©å……åˆ°åŸæœ‰çš„3å€å·¦å³ã€‚
The YOLO detection box / mask data augmentation script implements a series of random mixed enhancements
such as rotation, translation, scaling, horizontal flipping, brightness, and saturation,
and by default expands the dataset to about three times its original size.

usage:
    python yolo_augment.py \
                --input path/to/input \
                --output path/to/output \
                [--num 2]

folder:
    .
    â””â”€â”€ PATH_TO_input_folder
        â”œâ”€â”€ images
            â”œâ”€â”€ 1.jpg
            â”œâ”€â”€ 2.jpg
            â””â”€â”€ ...
        â”œâ”€â”€ labels
            â”œâ”€â”€ 1.txt
            â”œâ”€â”€ 2.txt
            â””â”€â”€ ...
    .
    â””â”€â”€ PATH_TO_output_folder
        â”œâ”€â”€ images
        â”œâ”€â”€ labels

"""

import os
import cv2
import shutil
import argparse
import numpy as np
import albumentations as A
from tqdm import tqdm
import concurrent.futures


def get_augmentation_pipelines():
    """å®šä¹‰å¹¶è¿”å›ç”¨äºæ£€æµ‹å’Œåˆ†å‰²çš„ä¸¤ç§å¢å¼ºç®¡é“"""
    shared_transforms = [
        A.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2, p=0.75),
        A.RandomBrightnessContrast(p=0.7),
        A.HorizontalFlip(p=0.7),
        A.VerticalFlip(p=0.7),
        A.Affine(
            scale=(0.8, 1.2),
            translate_percent=(-0.1, 0.1),
            # rotate=(-30, 30),
            p=0.75
        ),
    ]
    detection_pipeline = A.Compose(shared_transforms,
                                   bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))
    segmentation_pipeline = A.Compose(shared_transforms)
    return detection_pipeline, segmentation_pipeline


def process_single_image(args):
    """
    å¤„ç†å•å¼ å›¾ç‰‡çš„æ ¸å¿ƒå·¥ä½œå‡½æ•°ï¼Œå°†è¢«å¹¶è¡Œè°ƒç”¨ã€‚
    å®ƒæ¥æ”¶ä¸€ä¸ªå…ƒç»„ä½œä¸ºå‚æ•°ä»¥æ–¹ä¾¿åœ°ä¸ ProcessPoolExecutor é…åˆä½¿ç”¨ã€‚
    """
    # 1. è§£åŒ…å‚æ•°
    (image_filename, input_images_dir, input_labels_dir,
     output_images_dir, output_labels_dir, num_aug_per_image,
     detection_transform, segmentation_transform) = args

    base_name, ext = os.path.splitext(image_filename)
    image_path = os.path.join(input_images_dir, image_filename)
    label_path = os.path.join(input_labels_dir, base_name + '.txt')

    if not os.path.exists(label_path):
        return 0  # è¿”å›å¤„ç†ç»“æœï¼š0ä¸ªå¢å¼ºå›¾åƒè¢«åˆ›å»º

    # --- è‡ªåŠ¨æ ¼å¼æ£€æµ‹ ---
    current_format = None
    with open(label_path, 'r') as f:
        first_line = f.readline().strip()
        if first_line:
            parts = first_line.split()
            if len(parts) == 5:
                current_format = 'detection'
            elif len(parts) > 5 and len(parts) % 2 != 0:
                current_format = 'segmentation'

    if not current_format: return 0

    # --- è¯»å–æ•°æ®å’Œå¢å¼º ---
    image = cv2.imread(image_path)
    # image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    generated_count = 0
    for i in range(num_aug_per_image):
        aug_base_name = f"{base_name}_aug_{i}"
        try:
            if current_format == 'detection':
                bboxes, class_labels = [], []
                with open(label_path, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        class_labels.append(int(parts[0]))
                        bboxes.append([float(p) for p in parts[1:]])

                augmented = detection_transform(image=image, bboxes=bboxes, class_labels=class_labels)
                if not augmented['bboxes']: continue

                aug_image_path = os.path.join(output_images_dir, aug_base_name + ext)
                cv2.imwrite(aug_image_path, augmented['image'])

                aug_label_path = os.path.join(output_labels_dir, aug_base_name + '.txt')
                with open(aug_label_path, 'w') as f:
                    for bbox, class_id in zip(augmented['bboxes'], augmented['class_labels']):
                        f.write(f"{class_id} {' '.join(f'{c:.6f}' for c in bbox)}\n")
                generated_count += 1

            elif current_format == 'segmentation':
                h, w = image.shape[:2]
                masks, class_ids = [], []
                with open(label_path, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        class_ids.append(int(parts[0]))
                        polygon_norm = np.array([float(p) for p in parts[1:]]).reshape(-1, 2)
                        polygon_abs = polygon_norm * np.array([w, h])
                        mask = np.zeros((h, w), dtype=np.uint8)
                        cv2.fillPoly(mask, [polygon_abs.astype(np.int32)], 1)
                        masks.append(mask)

                augmented = segmentation_transform(image=image, masks=masks)
                new_h, new_w = augmented['image'].shape[:2]

                aug_image_path = os.path.join(output_images_dir, aug_base_name + ext)
                cv2.imwrite(aug_image_path, augmented['image'])

                aug_label_path = os.path.join(output_labels_dir, aug_base_name + '.txt')
                yolo_labels = []
                for class_id, aug_mask in zip(class_ids, augmented['masks']):
                    contours, _ = cv2.findContours(aug_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                    if contours:
                        contour = max(contours, key=cv2.contourArea)
                        if contour.shape[0] >= 3:
                            contour_norm = contour.astype(np.float32).squeeze(1)
                            contour_norm[:, 0] /= new_w
                            contour_norm[:, 1] /= new_h
                            yolo_line = f"{class_id} " + " ".join([f"{c:.6f}" for c in contour_norm.flatten()])
                            yolo_labels.append(yolo_line)

                if yolo_labels:
                    with open(aug_label_path, 'w') as f:
                        f.write("\n".join(yolo_labels))
                    generated_count += 1
        except Exception as e:
            # åœ¨å¹¶è¡Œç¯å¢ƒä¸­ï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯å¯èƒ½ä¼šè¢«æ‰“ä¹±ï¼Œä½†ä»ç„¶å¾ˆæœ‰ç”¨
            print(f"\nå¤„ç† {image_filename} æ—¶å‘ç”Ÿé”™è¯¯ (å¢å¼ºç‰ˆæœ¬ {i}): {e}")
    return generated_count


def augment_dataset_parallel(input_dir, output_dir, num_aug_per_image=1, num_threads=4):
    """
    ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†æ•°æ®é›†å¢å¼ºã€‚
    """
    # 1. ä¸²è¡Œéƒ¨åˆ†ï¼šè®¾ç½®è·¯å¾„ã€åˆ›å»ºç›®å½•ã€å¤åˆ¶æ–‡ä»¶
    input_images_dir = os.path.join(input_dir, 'images')
    input_labels_dir = os.path.join(input_dir, 'labels')
    output_images_dir = os.path.join(output_dir, 'images')
    output_labels_dir = os.path.join(output_dir, 'labels')

    os.makedirs(output_images_dir, exist_ok=True)
    os.makedirs(output_labels_dir, exist_ok=True)

    print("Step 1: æ­£åœ¨å¤åˆ¶åŸå§‹æ–‡ä»¶...")
    for subdir, output_subdir in [(input_images_dir, output_images_dir), (input_labels_dir, output_labels_dir)]:
        if not os.path.exists(subdir): continue
        for filename in tqdm(os.listdir(subdir), desc=f"å¤åˆ¶ {os.path.basename(subdir)}"):
            shutil.copy2(os.path.join(subdir, filename), os.path.join(output_subdir, filename))
    print("åŸå§‹æ–‡ä»¶å¤åˆ¶å®Œæˆï¼\n")

    # 2. å‡†å¤‡å¹¶è¡Œä»»åŠ¡
    print(f"Step 2: å‡†å¤‡ä»»åŠ¡åˆ—è¡¨ï¼Œå°†ä½¿ç”¨ {num_threads} ä¸ªCPUæ ¸å¿ƒå¹¶è¡Œå¤„ç†...")
    detection_transform, segmentation_transform = get_augmentation_pipelines()
    image_files = [f for f in os.listdir(input_images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

    # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰ä»»åŠ¡å‚æ•°çš„åˆ—è¡¨
    tasks = []
    for image_filename in image_files:
        tasks.append((
            image_filename, input_images_dir, input_labels_dir,
            output_images_dir, output_labels_dir, num_aug_per_image,
            detection_transform, segmentation_transform
        ))

    # 3. å¹¶è¡Œæ‰§è¡Œä»»åŠ¡
    total_generated = 0
    print(f"Step 3: å¼€å§‹å¹¶è¡Œå¤„ç† {len(tasks)} å¼ å›¾åƒ...")
    with concurrent.futures.ProcessPoolExecutor(max_workers=num_threads) as executor:
        # ä½¿ç”¨ executor.map æ¥åˆ†å‘ä»»åŠ¡ï¼Œå¹¶ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        results = list(tqdm(executor.map(process_single_image, tasks), total=len(tasks), desc="å¹¶è¡Œå¢å¼ºä¸­"))
        total_generated = sum(results)

    print(f"\næ•°æ®å¢å¼ºæµç¨‹å…¨éƒ¨å®Œæˆï¼")
    print(f"å…±ç”Ÿæˆäº† {total_generated} ä¸ªæ–°çš„å¢å¼ºæ ·æœ¬ã€‚")
    print(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åœ¨: {os.path.abspath(output_dir)}")


def main():
    parser = argparse.ArgumentParser(description="Intelligent YOLO Data Augmentation Script with Multi-Processing")
    parser.add_argument('--input', type=str, required=True,
                        help='Path to the input dataset directory.')
    parser.add_argument('--output', type=str, required=True,
                        help='Path to the directory to save the augmented dataset.')
    parser.add_argument('--num', type=int, default=2,
                        help='Number of augmented versions to generate per original image.')
    parser.add_argument('--threads', type=int, default=4,
                        help='Number of CPU cores to use for parallel processing.')

    args = parser.parse_args()
    augment_dataset_parallel(args.input, args.output, args.num, args.threads)

if __name__ == "__main__":
    main()